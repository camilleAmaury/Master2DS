{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet Data Camp\n",
    "\n",
    "This project can be found at \"./datacamp.pdf\". The aim is to find a way to maximize the \"cells\" size eaten at specific location within a limited range of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "In this section, we will import, open data and manage to explore and maybe reorganize them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datascience libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential, load_model\n",
    "\n",
    "# basic python libraries\n",
    "import math\n",
    "import time\n",
    "from tqdm.notebook import tqdm as pb\n",
    "\n",
    "# plot\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 16, 10\n",
    "\n",
    "# local imports\n",
    "from classes.Game import *\n",
    "from classes.Strategy import *\n",
    "from classes.Metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-11.03</td>\n",
       "      <td>-82.33</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-4.93</td>\n",
       "      <td>-98.05</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>81.52</td>\n",
       "      <td>2.41</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>36.82</td>\n",
       "      <td>-42.62</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-69.86</td>\n",
       "      <td>11.88</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0      1      2   3\n",
       "0  0 -11.03 -82.33  46\n",
       "1  1  -4.93 -98.05  96\n",
       "2  2  81.52   2.41  49\n",
       "3  3  36.82 -42.62  71\n",
       "4  4 -69.86  11.88  27"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Parser de fichiers\n",
    "df = pd.read_csv(\"data.txt\",header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     8.00\n",
       "1   -56.27\n",
       "2    23.81\n",
       "3    65.00\n",
       "Name: 8, dtype: float64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[8,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce Graph and store it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def ReduceGraph(df, neightbors):\n",
    "    def find_n_nearest_neightbors(M, position, range_, range2_):\n",
    "        temp = [([M[j,0], math.sqrt((M[position,1] - M[j,1])**2 + (M[position,2] - M[j,2])**2)]) for j in range2_]\n",
    "        # select the n best\n",
    "        temp.sort(key=lambda x: x[1])\n",
    "        return [temp[j] for j in range_]\n",
    "\n",
    "    temp = []\n",
    "    M = df.values\n",
    "    range_ = range(1,neightbors+1)\n",
    "    range2_ = range(M.shape[0])\n",
    "    with open('{}neightbors_matrix.json'.format(neightbors), 'w') as outfile:\n",
    "        json.dump([(find_n_nearest_neightbors(M, i, range_, range2_)) for i in pb(range2_)], outfile)\n",
    "        \n",
    "def LoadGraph(neightbors):\n",
    "    with open('{}neightbors_matrix.json'.format(neightbors)) as json_file:\n",
    "        return np.array(json.load(json_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ReduceGraph(df, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 30, 2)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_n_neightbors = 30\n",
    "new_df = LoadGraph(30)\n",
    "new_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## deep rl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    def __init__(self, max_size, input_shape, n_actions, discrete=False):\n",
    "        super(ReplayBuffer, self).__init__()\n",
    "        self.mem_size = max_size\n",
    "        self.discrete = discrete\n",
    "        self.state_memory = np.zeros((self.mem_size, input_shape))\n",
    "        self.new_state_memory = np.zeros((self.mem_size, input_shape))\n",
    "        dtype = np.int8 if self.discrete else np.float32\n",
    "        self.action_memory = np.zeros((self.mem_size, n_actions), dtype=dtype)\n",
    "        self.reward_memory = np.zeros(self.mem_size)\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
    "        self.mem_counter = 0\n",
    "        \n",
    "    def store_transition(self, state, action, reward, new_state, done):\n",
    "        index = self.mem_counter % self.mem_size\n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = new_state\n",
    "        self.reward_memory[index] = reward\n",
    "        self.terminal_memory[index] = 1 - int(done)\n",
    "        \n",
    "        if self.discrete:\n",
    "            actions = np.zeros(self.action_memory.shape[1])\n",
    "            actions[action] = 1.0\n",
    "            self.action_memory[index] = actions\n",
    "        else:\n",
    "            self.action_memory[index] = action\n",
    "        self.mem_counter += 1\n",
    "        \n",
    "    def sample_buffer(self, batch_size):\n",
    "        max_mem = min(self.mem_counter, self.mem_size)\n",
    "        batch = np.random.choice(max_mem, batch_size)\n",
    "        \n",
    "        states = self.state_memory[batch]\n",
    "        new_states = self.new_state_memory[batch]\n",
    "        rewards = self.reward_memory[batch]\n",
    "        actions = self.action_memory[batch]\n",
    "        terminal = self.terminal_memory[batch]\n",
    "        \n",
    "        return states, actions, rewards, new_states, terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, alpha, gamma, n_actions, epsilon, batch_size, input_dims,\n",
    "                fn, epsilon_decrease=0.0001, epsilon_min=0.0001, mem_size=1000000,\n",
    "                fname=\"dqn_model\"):\n",
    "        super(Agent, self).__init__()\n",
    "        self.action_space = [i for i in range(n_actions)]\n",
    "        self.n_actions = n_actions\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_dec = epsilon_decrease\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.batch_size = batch_size\n",
    "        self.model_file = fname\n",
    "        \n",
    "        self.memory = ReplayBuffer(mem_size, input_dims, n_actions, \n",
    "                                   discrete=True)\n",
    "        self.q_eval = fn(alpha, n_actions, input_dims, 128, 128)\n",
    "    \n",
    "    def remember(self, state, action, reward, new_state, done):\n",
    "        states_previous = np.array([-(state[i][1]**3/state[i][2]) if state[i][2] != 0. else -1000. for i in range(len(state))])\n",
    "        states_next = np.array([-(new_state[i][1]**3/new_state[i][2]) if new_state[i][2] != 0. else -1000. for i in range(len(state))])\n",
    "        self.memory.store_transition(states_previous, action, reward, states_next, done)\n",
    "        \n",
    "    def choose_action(self, state, time):\n",
    "        distances = np.array([state[i][1] for i in range(len(state))])\n",
    "        state_ = np.array([-(state[i][1]**3/state[i][2]) if state[i][2] != 0. else -1000. for i in range(len(state))])\n",
    "        state_ = state_[np.newaxis,:]\n",
    "        rand = np.random.random()\n",
    "        if rand < self.epsilon:\n",
    "            action = np.random.choice(self.action_space)\n",
    "        else:\n",
    "            actions = self.q_eval.predict(state_)\n",
    "            action = np.argmax(actions)\n",
    "            \n",
    "        if time - distances[action] < 0:\n",
    "            return (-1,-1)\n",
    "        else:\n",
    "            return (state[action][0], action)\n",
    "    \n",
    "    def learn(self):\n",
    "        if self.memory.mem_counter < self.batch_size:\n",
    "            return\n",
    "        state, action, reward, new_state, done = self.memory.sample_buffer(self.batch_size)\n",
    "        \n",
    "        action_values = np.array(self.action_space, dtype=np.int8)\n",
    "        action_indices = np.dot(action, action_values)\n",
    "        \n",
    "        q_eval = self.q_eval.predict(state)\n",
    "        q_next = self.q_eval.predict(new_state)\n",
    "        \n",
    "        q_target = q_eval.copy()\n",
    "        \n",
    "        batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
    "        q_target[batch_index, action_indices] = reward + self.gamma*np.max(q_next, axis=1)*done\n",
    "        \n",
    "        \n",
    "        _ = self.q_eval.fit(state, q_target, verbose=0)\n",
    "        \n",
    "        \n",
    "    def decrease(self):\n",
    "        self.epsilon = self.epsilon-self.epsilon_dec if self.epsilon > self.epsilon_min else self.epsilon_min\n",
    "    def save_model(self):\n",
    "        self.q_eval.save(self.model_file)\n",
    "    def load_model(self):\n",
    "        self.q_eval = load_model(self.model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game(object):\n",
    "    \n",
    "    def __init__(self, time, n):\n",
    "        super(Game, self).__init__()\n",
    "        self._const_time = time\n",
    "        self.n = n\n",
    "    \n",
    "    # init a game and reset it\n",
    "    def init_game(self, M, init_coord):\n",
    "        self._Values = M[:,1:]\n",
    "        self._Ids = M[:, 0].astype(int)\n",
    "        self.actual_coord = (init_coord[0], init_coord[1],-1)\n",
    "        self.ids = []\n",
    "        self.rewards = []\n",
    "        self.time = 0.\n",
    "        return self.find_n_nearest_neightbors(self.actual_coord)\n",
    "    \n",
    "    \n",
    "    def find_n_nearest_neightbors(self, position):\n",
    "        temp = []\n",
    "        for j in range(self._Values.shape[0]):\n",
    "            pos = (self._Ids[j], self._Values[j,0], self._Values[j,1], self._Values[j,2])\n",
    "            temp.append([pos[0], math.sqrt((position[0] - pos[1])**2 + (position[1] - pos[2])**2), pos[3]])\n",
    "        # select the n best\n",
    "        temp.sort(key=lambda x: x[1])\n",
    "        return [temp[j] for j in range(self.n)]\n",
    "\n",
    "    # step in a game\n",
    "    def step_env(self, action):\n",
    "        # check if there is a correct action or not\n",
    "        if action[1] == -1:\n",
    "            temp = np.zeros((self.n,3))\n",
    "            for i in range(temp.shape[0]):\n",
    "                temp[i,1]=0\n",
    "            return temp, 1, True, \"End of Game\"\n",
    "        else:\n",
    "            # get reward\n",
    "            distance = math.sqrt((self.actual_coord[0] - self._Values[action[0], 0])**2 + (self.actual_coord[1] - self._Values[action[0], 1])**2)\n",
    "            if self._Values[action[0], 2] == 0.:\n",
    "                previous_reward = -10000\n",
    "            else:\n",
    "                previous_reward = - (distance**3/self._Values[action[0], 2])\n",
    "            self.rewards.append(self._Values[action[0], 2])\n",
    "            self._Values[action[0], 2] = 0.\n",
    "\n",
    "            # keep track of action\n",
    "            self.ids.append(self._Ids[action[0]])\n",
    "            \n",
    "\n",
    "            # update time and coordinate\n",
    "            self.time += distance\n",
    "            self.actual_coord = (self._Values[action[0], 0], self._Values[action[0], 1], self._Ids[action[0]])\n",
    "\n",
    "            # step into the environnement\n",
    "            if self.time < self._const_time:\n",
    "                possibilities = self.find_n_nearest_neightbors(self.actual_coord)\n",
    "                return possibilities, previous_reward, False, \"Step\"\n",
    "            else:\n",
    "                temp = np.zeros((self.n,3))\n",
    "                for i in range(temp.shape[0]):\n",
    "                    temp[i,1]=0\n",
    "                # previous time was not correctly endled by the agent(strategy)\n",
    "                return temp, -10000, True, \"Time is out of bound\"\n",
    "    \n",
    "        \n",
    "    \n",
    "    # string representation of the object\n",
    "    def __repr__(self):\n",
    "        return \"<Object : Game (DataCamp) <> time:{}>\".format(self._const_time)\n",
    "    def __str__(self):\n",
    "        return self.__repr__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network(learning_rate, n_actions, input_dims, fc1_dims, fc2_dims):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(fc1_dims, input_shape=(input_dims,), activation='relu'))\n",
    "    model.add(Dense(fc2_dims, activation='relu'))\n",
    "    model.add(Dense(n_actions, activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer=\"adam\",loss=\"mse\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = df.values\n",
    "n = 100\n",
    "env = Game(10000., n)\n",
    "agent = Agent(gamma=0.90, epsilon=1.0, alpha=0.001, input_dims=n, n_actions=n,\n",
    "             fn=build_network,mem_size=10_000, batch_size=16)\n",
    "#agent.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c5b668d0169425798c08149103ad92c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-77-4806222757f6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_const_time\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m             \u001b[0mobservation_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_env\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremember\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobservation_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0mobservation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobservation_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-72-f060b8ccec53>\u001b[0m in \u001b[0;36mstep_env\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[1;31m# step into the environnement\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_const_time\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m                 \u001b[0mpossibilities\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_n_nearest_neightbors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactual_coord\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mpossibilities\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprevious_reward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Step\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-72-f060b8ccec53>\u001b[0m in \u001b[0;36mfind_n_nearest_neightbors\u001b[1;34m(self, position)\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[0mpos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Ids\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m             \u001b[0mtemp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mposition\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mposition\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[1;31m# select the n best\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mtemp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "_n_games = 100\n",
    "scores = []\n",
    "max_ = [0]\n",
    "for epoch in range(1):\n",
    "    agent.epsilon = 1.\n",
    "    agent.epsilon_decrease= 1./_n_games,\n",
    "    agent.epsilon_min= 1./_n_games\n",
    "    for i in pb(range(_n_games)):\n",
    "        done = False\n",
    "        observation = env.init_game(np.copy(M), (0,0))\n",
    "        while not done:\n",
    "            action = agent.choose_action(observation, env._const_time - env.time)\n",
    "            observation_, reward, done, info = env.step_env(action)\n",
    "            agent.remember(observation, action[1], reward, observation_, done)\n",
    "            observation = observation_\n",
    "            agent.learn()\n",
    "        agent.decrease()\n",
    "        if max_[0] < sum(env.rewards):\n",
    "            max_ = [sum(env.rewards), env.ids]\n",
    "        scores.append(sum(env.rewards))\n",
    "    agent.save_model() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save_model() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x23ab588c5c8>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA7IAAAI/CAYAAABK2eVGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAApEUlEQVR4nO3df5Af933f99c7AKKeWUlwJUgRALJkIhqtZDuEdOGw0cQZmbZApx4RddQYmbTijDWDRkMnTqZBYow7nnY6ra0gqRpNK6Ws5VBSLUsqDUJsxhSoRJ36H/0IaMgGKQsxWtEmcYxJVYKiNlcZhN/94/boO+oI3gEgvvgcHo+Zm9vv57t7/OzsAMTzdr+71d0BAACAUfyJWU8AAAAANkLIAgAAMBQhCwAAwFCELAAAAEMRsgAAAAxFyAIAADCUrbOewKV67Wtf2zfffPOspwEAAMDL4NFHH/16d+9Y671hQ/bmm2/OiRMnZj0NAAAAXgZV9Xsv9p5LiwEAABiKkAUAAGAoQhYAAIChCFkAAACGImQBAAAYipAFAABgKEIWAACAoQhZAAAAhiJkAQAAGIqQBQAAYChCFgAAgKEIWQAAAIYiZAEAABiKkAUAAGAoQhYAAIChCFkAAACGImQBAAAYipAFAABgKEIWAACAoQhZAAAAhrJ11hPYjI6dPJsjx09n4dxidm6fy6F9e7J/765ZTwsAAGBTELJX2LGTZ3P46Kksnr+QJDl7bjGHj55KEjELAABwBbi0+Ao7cvz08xG7bPH8hRw5fnpGMwIAANhchOwVtnBucUPjAAAAbIyQvcJ2bp/b0DgAAAAbI2SvsEP79mRu25ZVY3PbtuTQvj0zmhEAAMDm4mZPV9jyDZ3ctRgAAODlIWRfBvv37hKuAAAALxOXFgMAADAUIQsAAMBQhCwAAABDEbIAAAAMZV0hW1VPVNWpqvpyVZ2Yxv5sVX1+Gv/fqupVK9Y/XFVnqup0Ve1bMf7Waf0zVfWBqqpp/BVV9clp/ItVdfMV3k8AAAA2iY2ckX17d9/W3fPT619K8rPd/QNJHkxyKEmq6k1JDiR5c5K7knywqpYfrPqhJAeT3Dp93TWNvyfJN7v7jUnen+R9l75LAAAAbGaXc2nxniS/MS1/NslfnpbvTvKJ7v5Od38tyZkkt1fVG5K8qrs/392d5KNJ9q/Y5iPT8gNJ7lw+WwsAAAArrTdkO8kjVfVoVR2cxh5L8s5p+T9OcuO0vCvJkyu2fWoa2zUtv3B81Tbd/VySbyV5zfp3AwAAgOvFekP2bd39liQ/luTeqvqhJD81LT+a5JVJ/nBad60zqX2R8Ytts0pVHayqE1V14tlnn13n1AEAANhM1hWy3b0wfX8mS5+Hvb27v9rd7+jutyb51ST/57T6U/njs7NJsjvJwjS+e43xVdtU1dYkr07yjTXmcV93z3f3/I4dO9a3hwAAAGwqLxmyVXVDVb1yeTnJO5I8VlWvm8b+RJL/Isk/njZ5KMmB6U7Et2Tppk5f6u6nk3y7qu6YPv/67iSfXrHNPdPyu5J8bvocLQAAAKyydR3rvD7Jg9O9l7Ym+Xh3f6aqfqaq7p3WOZrknyRJdz9eVZ9K8pUkzyW5t7svTOu9N8n9SeaSPDx9JcmHk3ysqs5k6UzsgcvdMQAAADanGvXE5/z8fJ84cWLW0wAAAOBlUFWPrnj86yqX8/gdAAAAuOqELAAAAEMRsgAAAAxFyAIAADAUIQsAAMBQhCwAAABDEbIAAAAMRcgCAAAwFCELAADAUIQsAAAAQxGyAAAADEXIAgAAMBQhCwAAwFCELAAAAEMRsgAAAAxFyAIAADAUIQsAAMBQhCwAAABDEbIAAAAMRcgCAAAwFCELAADAUIQsAAAAQxGyAAAADEXIAgAAMBQhCwAAwFCELAAAAEMRsgAAAAxFyAIAADAUIQsAAMBQhCwAAABDEbIAAAAMRcgCAAAwlK2zngAAAABXx7GTZ3Pk+OksnFvMzu1zObRvT/bv3TXraW2YkAUAALgOHDt5NoePnsri+QtJkrPnFnP46KkkGS5mXVoMAABwHThy/PTzEbts8fyFHDl+ekYzunRCFgAA4DqwcG5xQ+PXMiELAABwHdi5fW5D49cyIQsAAHAdOLRvT+a2bVk1NrdtSw7t2zOjGV06N3sCAAC4Dizf0MldiwEAABjG/r27hgzXF3JpMQAAAEMRsgAAAAxFyAIAADAUIQsAAMBQhCwAAABDEbIAAAAMRcgCAAAwFCELAADAUIQsAAAAQxGyAAAADEXIAgAAMBQhCwAAwFCELAAAAEMRsgAAAAxFyAIAADAUIQsAAMBQhCwAAABDEbIAAAAMRcgCAAAwFCELAADAUNYVslX1RFWdqqovV9WJaey2qvrC8lhV3b5i/cNVdaaqTlfVvhXjb51+zpmq+kBV1TT+iqr65DT+xaq6+QrvJwAAAJvERs7Ivr27b+vu+en130/yX3X3bUl+fnqdqnpTkgNJ3pzkriQfrKot0zYfSnIwya3T113T+HuSfLO735jk/Uned8l7BAAAwKZ2OZcWd5JXTcuvTrIwLd+d5BPd/Z3u/lqSM0lur6o3JHlVd3++uzvJR5PsX7HNR6blB5LcuXy2FgAAAFbaus71OskjVdVJ/qfuvi/J30pyvKr+QZaC+M9P6+5K8oUV2z41jZ2fll84vrzNk0nS3c9V1beSvCbJ1ze6QwAAAGxu6w3Zt3X3QlW9Lslnq+qrSd6V5G93969V1V9J8uEkP5JkrTOpfZHxvMR7z6uqg1m6NDk33XTTOqcOAADAZrKuS4u7e2H6/kySB5PcnuSeJEenVf7XaSxZOtN644rNd2fpsuOnpuUXjq/apqq2ZulS5W+sMY/7unu+u+d37NixnqkDAACwybxkyFbVDVX1yuXlJO9I8liWIvQvTqv9cJLfnZYfSnJguhPxLVm6qdOXuvvpJN+uqjumz7++O8mnV2xzz7T8riSfmz5HCwAAAKus59Li1yd5cLr30tYkH+/uz1TV/5PkH01nUP+/TJf8dvfjVfWpJF9J8lySe7v7wvSz3pvk/iRzSR6evpKly5I/VlVnsnQm9sAV2DcAAAA2oRr1xOf8/HyfOHFi1tMAAADgZVBVj654/Osql/P4HQAAALjqhCwAAABDEbIAAAAMRcgCAAAwFCELAADAUIQsAAAAQ1nPc2RhTcdOns2R46ezcG4xO7fP5dC+Pdm/d9espwUAAGxyQpZLcuzk2Rw+eiqL5y8kSc6eW8zho6eSRMwCAAAvK5cWc0mOHD/9fMQuWzx/IUeOn57RjAAAgOuFkOWSLJxb3NA4AADAlSJkuSQ7t89taBwAAOBKEbJckkP79mRu25ZVY3PbtuTQvj0zmhEAAHC9cLMnLsnyDZ3ctRgAALjahCyXbP/eXddFuHrMEAAAXFuELFyExwwBAMC1x2dk4SI8ZggAAK49QhYuwmOGAADg2iNk4SI8ZggAAK49QhYuwmOGAADg2uNmT3ARHjMEAADXHiELL+F6ecwQAACMwqXFAAAADEXIAgAAMBQhCwAAwFCELAAAAEMRsgAAAAxFyAIAADAUIQsAAMBQhCwAAABDEbIAAAAMRcgCAAAwFCELAADAUIQsAAAAQxGyAAAADEXIAgAAMBQhCwAAwFCELAAAAEMRsgAAAAxFyAIAADAUIQsAAMBQhCwAAABDEbIAAAAMRcgCAAAwFCELAADAUIQsAAAAQxGyAAAADEXIAgAAMBQhCwAAwFCELAAAAEMRsgAAAAxFyAIAADAUIQsAAMBQhCwAAABD2TrrCQBcTcdOns2R46ezcG4xO7fP5dC+Pdm/d9espwUAwAYIWeC6cezk2Rw+eiqL5y8kSc6eW8zho6eSRMwCsGF+OQqz49Ji4Lpx5Pjp5yN22eL5Czly/PSMZgTAqJZ/OXr23GI6f/zL0WMnz856anBdELLAdWPh3OKGxgHgxfjlKMyWkAWuGzu3z21oHABejF+OwmwJWeC6cWjfnsxt27JqbG7blhzat2dGMwJgVH45CrMlZIHrxv69u/ILP/ED2bV9LpVk1/a5/MJP/IAbcwCwYX45CrO1rrsWV9UTSb6d5EKS57p7vqo+mWT5T+r2JOe6+7Zp/cNJ3jOt/ze7+/g0/tYk9yeZS/LrSX6mu7uqXpHko0nemuT/TvKT3f3E5e8ewGr79+4SrgBctuX/l7hrMczGRh6/8/bu/vryi+7+yeXlqvqHSb41Lb8pyYEkb06yM8k/q6rv6+4LST6U5GCSL2QpZO9K8nCWoveb3f3GqjqQ5H1Jnv/5AABwrfHLUZidy760uKoqyV9J8qvT0N1JPtHd3+nuryU5k+T2qnpDkld19+e7u7N0Bnb/im0+Mi0/kOTO6ecCAADAKusN2U7ySFU9WlUHX/DeX0jyB939u9PrXUmeXPH+U9PYrmn5heOrtunu57J0dvc1690JAAAArh/rvbT4bd29UFWvS/LZqvpqd//G9N5fzR+fjU2Stc6k9kXGL7bNKlNEH0ySm266aZ1TBwAAYDNZ1xnZ7l6Yvj+T5MEktydJVW1N8hNJPrli9aeS3Lji9e4kC9P47jXGV20z/cxXJ/nGGvO4r7vnu3t+x44d65k6AAAAm8xLhmxV3VBVr1xeTvKOJI9Nb/9Ikq9298pLhh9KcqCqXlFVtyS5NcmXuvvpJN+uqjumz7++O8mnV2xzz7T8riSfmz5HCwAAAKus59Li1yd5cLr30tYkH+/uz0zvHcjqy4rT3Y9X1aeSfCXJc0nune5YnCTvzR8/fufh6StJPpzkY1V1JktnYg9c6g4BAACwudWoJz7n5+f7xIkTs54GAAAAL4OqerS759d677IfvwMAAABXk5AFAABgKEIWAACAoQhZAAAAhiJkAQAAGIqQBQAAYChCFgAAgKEIWQAAAIYiZAEAABiKkAUAAGAoQhYAAIChCFkAAACGImQBAAAYipAFAABgKEIWAACAoWyd9QQAAI6dPJsjx09n4dxidm6fy6F9e7J/765ZTwuAa5SQBQBm6tjJszl89FQWz19Ikpw9t5jDR08liZgFYE0uLQYAZurI8dPPR+yyxfMXcuT46RnNCIBrnZAFAGZq4dzihsYBQMgCADO1c/vchsYBQMgCADN1aN+ezG3bsmpsbtuWHNq3Z0YzAuBa52ZPAMBMLd/QyV2LAVgvIQsAzNz+vbuEKwDrJmQBAIAX5TnPXIuELAAAsCbPeeZa5WZPAADAmjznmWuVkAUAANbkOc9cq4QsAACwJs955lolZAEAgDV5zjPXKjd7AgAA1uQ5z1yrhCwAAPCiPOeZa5FLiwEAABiKkAUAAGAoQhYAAIChCFkAAACGImQBAAAYipAFAABgKEIWAACAoXiOLAAAcN07dvJsjhw/nYVzi9m5fS6H9u3x/NxrmJAFAACua8dOns3ho6eyeP5CkuTsucUcPnoqScTsNcqlxQAAwHXtyPHTz0fsssXzF3Lk+OkZzYiXImQBAIDr2sK5xQ2NM3tCFgAAuK7t3D63oXFmT8gCAADXtUP79mRu25ZVY3PbtuTQvj0zmhEvxc2eAACA69ryDZ3ctXgcQhYAALju7d+7S7gOxKXFAAAADEXIAgAAMBQhCwAAwFCELAAAAEMRsgAAAAxFyAIAADAUIQsAAMBQhCwAAABDEbIAAAAMRcgCAAAwFCELAADAULbOegLAteHYybM5cvx0Fs4tZuf2uRzatyf79+6a9bQAAOC7CFkgx06ezeGjp7J4/kKS5Oy5xRw+eipJxCwAANcclxYDOXL89PMRu2zx/IUcOX56RjMCAIAXt66QraonqupUVX25qk6sGP8bVXW6qh6vqr+/YvxwVZ2Z3tu3Yvyt0885U1UfqKqaxl9RVZ+cxr9YVTdfwX0EXsLCucUNjQMAwCxt5NLit3f315dfVNXbk9yd5Ae7+ztV9bpp/E1JDiR5c5KdSf5ZVX1fd19I8qEkB5N8IcmvJ7krycNJ3pPkm939xqo6kOR9SX7ysvcOWJed2+dydo1o3bl9bgazAQCAi7ucS4vfm+QXu/s7SdLdz0zjdyf5RHd/p7u/luRMktur6g1JXtXdn+/uTvLRJPtXbPORafmBJHcun60FXn6H9u3J3LYtq8bmtm3JoX17ZjQjAAB4cesN2U7ySFU9WlUHp7HvS/IXpkuB/4+q+nPT+K4kT67Y9qlpbNe0/MLxVdt093NJvpXkNRvdGeDS7N+7K7/wEz+QXdvnUkl2bZ/LL/zED7jREwAA16T1Xlr8tu5emC4f/mxVfXXa9nuT3JHkzyX5VFX96SRrnUnti4znJd573hTRB5PkpptuWufUgfXYv3eXcAUAYAjrOiPb3QvT92eSPJjk9iydUT3aS76U5I+SvHYav3HF5ruTLEzju9cYz8ptqmprklcn+cYa87ivu+e7e37Hjh3r3UcAAAA2kZcM2aq6oapeubyc5B1JHktyLMkPT+Pfl+RPJvl6koeSHJjuRHxLkluTfKm7n07y7aq6Y/r867uTfHr6zzyU5J5p+V1JPjd9jhYAAABWWc+lxa9P8uB076WtST7e3Z+pqj+Z5Jer6rEkf5jknik+H6+qTyX5SpLnktw73bE4WbpB1P1J5rJ0t+KHp/EPJ/lYVZ3J0pnYA1di5wAAANh8atQTn/Pz833ixImXXhEAAIDhVNWj3T2/1nsbeY4sAIM4dvJsjhw/nYVzi9m5fS6H9u1xM69BOZYA8N2ELMAmc+zk2Rw+eiqL55c+1XH23GIOHz2VJAJoMI4lAKxtvc+RBWAQR46ffj58li2ev5Ajx0/PaEZcKscSANYmZAE2mYVzixsa59rlWALA2oQswCazc/vchsa5djmWALA2IQuwyRzatydz27asGpvbtiWH9u2Z0Yy4VI4lAKzNzZ4ANpnlmwC50+34HEsAWJvnyAIAAHDNudhzZF1aDAAAwFCELAAAAEMRsgAAAAxFyAIAADAUIQsAAMBQhCwAAABDEbIAAAAMRcgCAAAwFCELAADAUIQsAAAAQxGyAAAADEXIAgAAMBQhCwAAwFCELAAAAEMRsgAAAAxFyAIAADAUIQsAAMBQhCwAAABDEbIAAAAMRcgCAAAwFCELAADAUIQsAAAAQxGyAAAADEXIAgAAMBQhCwAAwFCELAAAAEMRsgAAAAxFyAIAADAUIQsAAMBQhCwAAABDEbIAAAAMRcgCAAAwFCELAADAUIQsAAAAQxGyAAAADEXIAgAAMBQhCwAAwFCELAAAAEMRsgAAAAxFyAIAADAUIQsAAMBQhCwAAABDEbIAAAAMRcgCAAAwFCELAADAUIQsAAAAQxGyAAAADEXIAgAAMBQhCwAAwFCELAAAAEMRsgAAAAxFyAIAADAUIQsAAMBQ1hWyVfVEVZ2qqi9X1Ylp7L+sqrPT2Jer6i+tWP9wVZ2pqtNVtW/F+Funn3Omqj5QVTWNv6KqPjmNf7Gqbr7C+wkAAMAmsZEzsm/v7tu6e37F2Punsdu6+9eTpKrelORAkjcnuSvJB6tqy7T+h5IcTHLr9HXXNP6eJN/s7jcmeX+S913yHgEAALCpvRyXFt+d5BPd/Z3u/lqSM0lur6o3JHlVd3++uzvJR5PsX7HNR6blB5LcuXy2FgAAAFZab8h2kkeq6tGqOrhi/Ker6rer6per6nunsV1JnlyxzlPT2K5p+YXjq7bp7ueSfCvJaza0JwAAAFwX1huyb+vutyT5sST3VtUPZeky4T+T5LYkTyf5h9O6a51J7YuMX2ybVarqYFWdqKoTzz777DqnDgAAwGayrpDt7oXp+zNJHkxye3f/QXdf6O4/SvI/J7l9Wv2pJDeu2Hx3koVpfPca46u2qaqtSV6d5BtrzOO+7p7v7vkdO3asbw8BAADYVF4yZKvqhqp65fJyknckeWz6zOuy/yjJY9PyQ0kOTHciviVLN3X6Unc/neTbVXXH9PnXdyf59Ipt7pmW35Xkc9PnaAEAAGCVretY5/VJHpzuvbQ1yce7+zNV9bGqui1LlwA/keQ/S5LufryqPpXkK0meS3Jvd1+YftZ7k9yfZC7Jw9NXknw4yceq6kyWzsQeuOw9AwAAYFOqUU98zs/P94kTJ2Y9DQAAAF4GVfXoCx7/+ryX4/E7AAAA8LIRsgAAAAxFyAIAADAUIQsAAMBQhCwAAABDEbIAAAAMRcgCAAAwFCELAADAUIQsAAAAQxGyAAAADEXIAgAAMBQhCwAAwFCELAAAAEMRsgAAAAxFyAIAADAUIQsAAMBQhCwAAABDEbIAAAAMRcgCAAAwFCELAADAUIQsAAAAQxGyAAAADEXIAgAAMBQhCwAAwFCELAAAAEMRsgAAAAxFyAIAADAUIQsAAMBQhCwAAABDEbIAAAAMRcgCAAAwFCELAADAUIQsAAAAQxGyAAAADEXIAgAAMBQhCwAAwFCELAAAAEMRsgAAAAxFyAIAADAUIQsAAMBQhCwAAABDEbIAAAAMZeusJwAAl+rYybM5cvx0Fs4tZuf2uRzatyf79+6a9bQAgJeZkAVgSMdOns3ho6eyeP5CkuTsucUcPnoqScQsAGxyLi0GYEhHjp9+PmKXLZ6/kCPHT89oRgDA1SJkARjSwrnFDY0DAJuHkAVgSDu3z21oHADYPIQsAEM6tG9P5rZtWTU2t21LDu3bM6MZAQBXi5s9ATCk5Rs6uWsxAFx/hCwAw9q/d5dwBYDrkEuLAQAAGIqQBQAAYChCFgAAgKEIWQAAAIYiZAEAABiKkAUAAGAoQhYAAIChCFkAAACGImQBAAAYipAFAABgKOsK2ap6oqpOVdWXq+rEC977O1XVVfXaFWOHq+pMVZ2uqn0rxt86/ZwzVfWBqqpp/BVV9clp/ItVdfMV2j8AAAA2mY2ckX17d9/W3fPLA1V1Y5IfTfL7K8belORAkjcnuSvJB6tqy/T2h5IcTHLr9HXXNP6eJN/s7jcmeX+S913a7gAAALDZXe6lxe9P8neT9Iqxu5N8oru/091fS3Imye1V9YYkr+ruz3d3J/lokv0rtvnItPxAkjuXz9YCAADASusN2U7ySFU9WlUHk6Sq3pnkbHf/1gvW3ZXkyRWvn5rGdk3LLxxftU13P5fkW0les4H9AAAA4DqxdZ3rva27F6rqdUk+W1VfTfJzSd6xxrprnUnti4xfbJvVP3gpog8myU033bSeeQMAALDJrOuMbHcvTN+fSfJgkr+Y5JYkv1VVTyTZneQ3q+pPZelM640rNt+dZGEa373GeFZuU1Vbk7w6yTfWmMd93T3f3fM7duxY5y4CAACwmbxkyFbVDVX1yuXlLJ2F/Rfd/bruvrm7b85SiL6lu/9VkoeSHJjuRHxLlm7q9KXufjrJt6vqjunzr+9O8unpP/NQknum5Xcl+dz0OVoAAABYZT2XFr8+yYPTvZe2Jvl4d3/mxVbu7ser6lNJvpLkuST3dveF6e33Jrk/yVySh6evJPlwko9V1ZksnYk9sPFdAQAA4HpQo574nJ+f7xMnTrz0igAAAAynqh5d+fjXlS738TsAAABwVQlZAAAAhiJkAQAAGIqQBQAAYChCFgAAgKEIWQAAAIYiZAEAABiKkAUAAGAoQhYAAIChCFkAAACGImQBAAAYipAFAABgKEIWAACAoQhZAAAAhiJkAQAAGIqQBQAAYChCFgAAgKEIWQAAAIYiZAEAABiKkAUAAGAoQhYAAIChbJ31BAAArhfHTp7NkeOns3BuMTu3z+XQvj3Zv3fXrKcFMBwhCwBwFRw7eTaHj57K4vkLSZKz5xZz+OipJBGzABvk0mIAgKvgyPHTz0fsssXzF3Lk+OkZzQhgXEIWAOAqWDi3uKFxAF6ckAUAuAp2bp/b0DgAL07IAgBcBYf27cncti2rxua2bcmhfXtmNCOAcbnZEwDAVbB8Qyd3LQa4fEIWAOAq2b93l3AFuAJcWgwAAMBQhCwAAABDEbIAAAAMRcgCAAAwFCELAADAUIQsAAAAQxGyAAAADEXIAgAAMBQhCwAAwFCELAAAAEMRsgAAAAxFyAIAADAUIQsAAMBQhCwAAABDEbIAAAAMRcgCAAAwFCELAADAUIQsAAAAQxGyAAAADEXIAgAAMBQhCwAAwFCELAAAAEMRsgAAAAxFyAIAADAUIQsAAMBQhCwAAABDEbIAAAAMRcgCAAAwFCELAADAUIQsAAAAQxGyAAAADGVdIVtVT1TVqar6clWdmMb+66r67WnskarauWL9w1V1pqpOV9W+FeNvnX7Omar6QFXVNP6KqvrkNP7Fqrr5Cu8nAAAAm8RGzsi+vbtv6+756fWR7v7B7r4tyT9N8vNJUlVvSnIgyZuT3JXkg1W1ZdrmQ0kOJrl1+rprGn9Pkm929xuTvD/J+y59lwAAANjMLvnS4u7+1yte3pCkp+W7k3yiu7/T3V9LcibJ7VX1hiSv6u7Pd3cn+WiS/Su2+ci0/ECSO5fP1gIAAMBK6w3ZTvJIVT1aVQeXB6vqv6mqJ5P8tUxnZJPsSvLkim2fmsZ2TcsvHF+1TXc/l+RbSV6zsV0BAADgerDekH1bd78lyY8lubeqfihJuvvnuvvGJL+S5Kenddc6k9oXGb/YNqtU1cGqOlFVJ5599tl1Th0AAIDNZF0h290L0/dnkjyY5PYXrPLxJH95Wn4qyY0r3tudZGEa373G+Kptqmprklcn+cYa87ivu+e7e37Hjh3rmToAAACbzEuGbFXdUFWvXF5O8o4kj1XVrStWe2eSr07LDyU5MN2J+JYs3dTpS939dJJvV9Ud0+df353k0yu2uWdafleSz02fowUAAIBVtq5jndcneXC699LWJB/v7s9U1a9V1Z4kf5Tk95L89STp7ser6lNJvpLkuST3dveF6We9N8n9SeaSPDx9JcmHk3ysqs5k6UzsgSuwbwAAAGxCNeqJz/n5+T5x4sSspwEAAMDLoKoeXfH411Uu+fE7AAAAMAtCFgAAgKEIWQAAAIYiZAEAABiKkAUAAGAoQhYAAIChCFkAAACGImQBAAAYipAFAABgKEIWAACAoQhZAAAAhiJkAQAAGIqQBQAAYChCFgAAgKEIWQAAAIYiZAEAABiKkAUAAGAoQhYAAIChCFkAAACGImQBAAAYipAFAABgKEIWAACAoQhZAAAAhiJkAQAAGIqQBQAAYChCFgAAgKEIWQAAAIYiZAEAABiKkAUAAGAoQhYAAIChCFkAAACGImQBAAAYipAFAABgKEIWAACAoQhZAAAAhiJkAQAAGIqQBQAAYChCFgAAgKEIWQAAAIYiZAEAABiKkAUAAGAoQhYAAIChCFkAAACGImQBAAAYipAFAABgKEIWAACAoQhZAAAAhiJkAQAAGIqQBQAAYChCFgAAgKEIWQAAAIYiZAEAABiKkAUAAGAoQhYAAIChCFkAAACGImQBAAAYipAFAABgKEIWAACAoWyd9QQAANhcjp08myPHT2fh3GJ2bp/LoX17sn/vrllPC9hEhCwAAFfMsZNnc/joqSyev5AkOXtuMYePnkoSMQtcMeu6tLiqnqiqU1X15ao6MY0dqaqvVtVvV9WDVbV9xfqHq+pMVZ2uqn0rxt86/ZwzVfWBqqpp/BVV9clp/ItVdfOV3U0AAK6GI8dPPx+xyxbPX8iR46dnNCNgM9rIZ2Tf3t23dff89PqzSb6/u38wyb9McjhJqupNSQ4keXOSu5J8sKq2TNt8KMnBJLdOX3dN4+9J8s3ufmOS9yd536XvEgAAs7JwbnFD4wCX4pJv9tTdj3T3c9PLLyTZPS3fneQT3f2d7v5akjNJbq+qNyR5VXd/vrs7yUeT7F+xzUem5QeS3Ll8thYAgHHs3D63oXGAS7HekO0kj1TVo1V1cI33fyrJw9PyriRPrnjvqWls17T8wvFV20xx/K0kr1nn3AAAuEYc2rcnc9u2rBqb27Ylh/btmdGMgM1ovTd7elt3L1TV65J8tqq+2t2/kSRV9XNJnkvyK9O6a51J7YuMX2ybVaaIPpgkN9100zqnDgDA1bJ8Qyd3LQZeTusK2e5emL4/U1UPJrk9yW9U1T1JfjzJndPlwsnSmdYbV2y+O8nCNL57jfGV2zxVVVuTvDrJN9aYx31J7kuS+fn57wpdAABmb//eXcIVeFm95KXFVXVDVb1yeTnJO5I8VlV3Jfl7Sd7Z3f9mxSYPJTkw3Yn4lizd1OlL3f10km9X1R3T51/fneTTK7a5Z1p+V5LPrQhjAAAAeN56zsi+PsmD072Xtib5eHd/pqrOJHlFli41TpIvdPdf7+7Hq+pTSb6SpUuO7+3u5XuwvzfJ/UnmsvSZ2uXP1X44ycemn/mNLN31GAAAAL5LjXric35+vk+cODHraQAAAPAyqKpHVzz+dZVLfvwOAAAAzIKQBQAAYChCFgAAgKEIWQAAAIYiZAEAABiKkAUAAGAoQhYAAIChCFkAAACGImQBAAAYipAFAABgKEIWAACAoQhZAAAAhiJkAQAAGIqQBQAAYChCFgAAgKEIWQAAAIYiZAEAABhKdfes53BJqurZJL8363m8hNcm+fqsJ8EV43huLo7n5uJ4bh6O5ebieG4ujufmMsLx/He7e8dabwwbsiOoqhPdPT/reXBlOJ6bi+O5uTiem4djubk4npuL47m5jH48XVoMAADAUIQsAAAAQxGyL6/7Zj0BrijHc3NxPDcXx3PzcCw3F8dzc3E8N5ehj6fPyAIAADAUZ2QBAAAYipB9GVTVXVV1uqrOVNXPzno+XLqqurGq/veq+p2qeryqfmbWc+LyVdWWqjpZVf901nPh8lTV9qp6oKq+Ov05/Q9mPScuXVX97env2seq6ler6t+a9ZxYv6r65ap6pqoeWzH271TVZ6vqd6fv3zvLObJ+L3I8j0x/3/52VT1YVdtnOEU2YK3jueK9v1NVXVWvncXcLpWQvcKqakuS/zHJjyV5U5K/WlVvmu2suAzPJfnPu/vfT3JHknsdz03hZ5L8zqwnwRXxj5J8prv/vSR/No7rsKpqV5K/mWS+u78/yZYkB2Y7Kzbo/iR3vWDsZ5P88+6+Nck/n14zhvvz3cfzs0m+v7t/MMm/THL4ak+KS3Z/vvt4pqpuTPKjSX7/ak/ocgnZK+/2JGe6+//q7j9M8okkd894Tlyi7n66u39zWv52lv6RvGu2s+JyVNXuJP9hkl+a9Vy4PFX1qiQ/lOTDSdLdf9jd52Y6KS7X1iRzVbU1yfckWZjxfNiA7v6NJN94wfDdST4yLX8kyf6rOScu3VrHs7sf6e7nppdfSLL7qk+MS/Iifz6T5P1J/m6S4W6cJGSvvF1Jnlzx+qkIn02hqm5OsjfJF2c8FS7Pf5+lv7D/aMbz4PL96STPJvkn06Xiv1RVN8x6Ulya7j6b5B9k6azA00m+1d2PzHZWXAGv7+6nk6VfDid53Yznw5XzU0kenvUkuHRV9c4kZ7v7t2Y9l0shZK+8WmNsuN9wsFpV/dtJfi3J3+rufz3r+XBpqurHkzzT3Y/Oei5cEVuTvCXJh7p7b5L/Ny5bHNb02cm7k9ySZGeSG6rqP5ntrIC1VNXPZenjV78y67lwaarqe5L8XJKfn/VcLpWQvfKeSnLjite749KooVXVtixF7K9099FZz4fL8rYk76yqJ7J02f8PV9X/MtspcRmeSvJUdy9fJfFAlsKWMf1Ikq9197PdfT7J0SR/fsZz4vL9QVW9IUmm78/MeD5cpqq6J8mPJ/lr7TmeI/szWfrF4W9N/y7aneQ3q+pPzXRWGyBkr7x/keTWqrqlqv5klm5U8dCM58QlqqrK0ufvfqe7/7tZz4fL092Hu3t3d9+cpT+bn+tuZ3wG1d3/KsmTVbVnGrozyVdmOCUuz+8nuaOqvmf6u/fOuHnXZvBQknum5XuSfHqGc+EyVdVdSf5eknd297+Z9Xy4dN19qrtf1903T/8ueirJW6b/tw5ByF5h0wfgfzrJ8Sz9D/hT3f34bGfFZXhbkv80S2fuvjx9/aVZTwp43t9I8itV9dtJbkvy3852Olyq6cz6A0l+M8mpLP0b5b6ZTooNqapfTfL5JHuq6qmqek+SX0zyo1X1u1m6M+ovznKOrN+LHM//Ickrk3x2+jfRP57pJFm3FzmeQytXBAAAADASZ2QBAAAYipAFAABgKEIWAACAoQhZAAAAhiJkAQAAGIqQBQAAYChCFgAAgKEIWQAAAIby/wMJiOsj1YrrcAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1152x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(np.arange(0, len(scores)),scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(gamma=0.90, epsilon=0., alpha=0.001, input_dims=_n_neightbors, n_actions=_n_neightbors,\n",
    "             fn=build_network,mem_size=100_000, batch_size=750)\n",
    "agent.epsilon_decrease= 0.\n",
    "agent.epsilon_min= 0.\n",
    "agent.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188.0 15414\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "observation = env.init_game(np.copy(M), (0,0))\n",
    "while not done:\n",
    "    action = agent.choose_action(observation, env._const_time - env.time)\n",
    "    observation_, reward, done, info = env.step_env(action)\n",
    "    agent.remember(observation, action[1], reward, observation_, done)\n",
    "    observation = observation_\n",
    "print(sum(env.rewards), len(env.ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2455 65925.0\n"
     ]
    }
   ],
   "source": [
    "print(len(max_[1]), max_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_result(M, ids):\n",
    "    M_temp = np.copy(M)\n",
    "    rewards = []\n",
    "    dist = 10000.\n",
    "    coordinate = (0.,0.)\n",
    "    for i in ids:\n",
    "        rewards.append(M_temp[i, 3])\n",
    "        M_temp[i, 3] = 0.\n",
    "        dist -= math.sqrt((coordinate[0] - M_temp[i, 1])**2 + (coordinate[1] - M_temp[i, 2])**2)\n",
    "        coordinate = (M_temp[i, 1],M_temp[i, 2])\n",
    "        \n",
    "    print(dist, sum(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5349012459854574 65925.0\n"
     ]
    }
   ],
   "source": [
    "check_result(M,max_[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
